{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d37cc850-7cd0-4810-8b0c-cf81bc4c0862",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 643 mobile_elements.csv files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/slurm.14551884/ipykernel_1264651/248863132.py:28: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  mobile_df = pd.concat(all_data, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Merged file saved to /nobackup/zxww47/merged_mge.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "mobileelements_dir = Path(\"/nobackup/zxww47/flye_mobile_elements_copy\")\n",
    "metadata_file = Path(\"/nobackup/zxww47/SraRunTable.csv\")\n",
    "output_file = Path(\"/nobackup/zxww47/merged_mge.csv\")\n",
    "\n",
    "# Load metadata\n",
    "metadata_df = pd.read_csv(metadata_file)\n",
    "\n",
    "# List all mobile_elements CSVs\n",
    "mobile_csvs = list(mobileelements_dir.glob(\"*_mobile_elements.csv\"))\n",
    "print(f\"Found {len(mobile_csvs)} mobile_elements.csv files\")\n",
    "\n",
    "# Read and merge them\n",
    "all_data = []\n",
    "for csv_file in mobile_csvs:\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        accession = csv_file.stem.replace(\"_mobile_elements\", \"\")  # extract Run code\n",
    "        df.insert(0, \"Run\", accession)\n",
    "        all_data.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error reading {csv_file}: {e}\")\n",
    "\n",
    "# Combine into one DataFrame\n",
    "mobile_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Merge with metadata\n",
    "merged_df = pd.merge(mobile_df, metadata_df, on=\"Run\", how=\"left\")\n",
    "\n",
    "# Save merged data\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "print(f\"âœ… Merged file saved to {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c783c74d-ad6e-4e1c-9e09-9d0e817b4736",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Processing plasmidfinder files...\n",
      "Found 596 files for plasmidfinder.\n",
      "âœ… Combined file saved: /nobackup/zxww47/Plasmids/plasmidfinder_combined.csv\n",
      "ðŸ”¹ Processing resfinder files...\n",
      "Found 596 files for resfinder.\n",
      "âœ… Combined file saved: /nobackup/zxww47/Resfinder/resfinder_combined.csv\n",
      "ðŸ”¹ Processing vfdb files...\n",
      "Found 596 files for vfdb.\n",
      "âœ… Combined file saved: /nobackup/zxww47/Vfdb/vfdb_combined.csv\n",
      "ðŸŽ‰ All processing complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Base paths\n",
    "base_dir = Path(\"/nobackup/zxww47/flye\")\n",
    "output_base = Path(\"/nobackup/zxww47\")\n",
    "metadata_file = output_base / \"SraRunTable.csv\"\n",
    "\n",
    "# Output folders\n",
    "db_map = {\n",
    "    \"plasmidfinder\": output_base / \"Plasmids\",\n",
    "    \"resfinder\": output_base / \"Resfinder\",\n",
    "    \"vfdb\": output_base / \"Vfdb\",\n",
    "}\n",
    "\n",
    "# Ensure output dirs exist\n",
    "for folder in db_map.values():\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load metadata\n",
    "metadata = pd.read_csv(metadata_file, dtype=str)\n",
    "\n",
    "# Helper: extract run code from filename (before first \"_\")\n",
    "def extract_run_code(filename: str) -> str:\n",
    "    return Path(filename).stem.split(\"_\")[0]\n",
    "\n",
    "# Process each db type\n",
    "for db_name, out_dir in db_map.items():\n",
    "    print(f\"ðŸ”¹ Processing {db_name} files...\")\n",
    "\n",
    "    # Find all TSV files matching this db\n",
    "    tsv_files = list(base_dir.rglob(f\"*_{db_name}_abricate.tsv\"))\n",
    "    print(f\"Found {len(tsv_files)} files for {db_name}.\")\n",
    "\n",
    "    dfs = []\n",
    "    for tsv_file in tsv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(tsv_file, sep=\"\\t\", dtype=str)  # keep everything as text\n",
    "\n",
    "            # Drop #FILE column if present\n",
    "            if \"#FILE\" in df.columns:\n",
    "                df = df.drop(columns=[\"#FILE\"])\n",
    "\n",
    "            # Add run code column\n",
    "            run_code = extract_run_code(tsv_file.name)\n",
    "            df.insert(0, \"Run\", run_code)\n",
    "\n",
    "            # Save individual CSV to correct folder\n",
    "            csv_file = out_dir / (tsv_file.stem + \".csv\")\n",
    "            df.to_csv(csv_file, index=False)\n",
    "\n",
    "            dfs.append(df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error processing {tsv_file}: {e}\")\n",
    "\n",
    "    # Combine into one CSV\n",
    "    if dfs:\n",
    "        combined = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "        # Merge with metadata (both use \"Run\")\n",
    "        merged = combined.merge(metadata, on=\"Run\", how=\"left\")\n",
    "\n",
    "        # Save combined + merged file\n",
    "        combined_file = out_dir / f\"{db_name}_combined.csv\"\n",
    "        merged.to_csv(combined_file, index=False)\n",
    "\n",
    "        print(f\"âœ… Combined file saved: {combined_file}\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ No valid files found for {db_name}.\")\n",
    "\n",
    "print(\"ðŸŽ‰ All processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5ec1a92-2552-488b-aa00-4c3f080e2813",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "OllamaEmbeddingFunction.__init__() got an unexpected keyword argument 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m client \u001b[38;5;241m=\u001b[39m chromadb\u001b[38;5;241m.\u001b[39mPersistentClient(path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchroma_storage_nomic\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Nomic embeddings\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m nomic_ef \u001b[38;5;241m=\u001b[39m \u001b[43membedding_functions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOllamaEmbeddingFunction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnomic-embed-text\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttp://127.0.0.1:11434/api/embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate_collection\u001b[39m(name):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m client\u001b[38;5;241m.\u001b[39mcreate_collection(name\u001b[38;5;241m=\u001b[39mname, embedding_function\u001b[38;5;241m=\u001b[39mnomic_ef)\n",
      "\u001b[0;31mTypeError\u001b[0m: OllamaEmbeddingFunction.__init__() got an unexpected keyword argument 'model'"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "import pandas as pd\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# Persistent client\n",
    "client = chromadb.PersistentClient(path=\"chroma_storage_nomic\")\n",
    "\n",
    "# Nomic embeddings\n",
    "nomic_ef = embedding_functions.OllamaEmbeddingFunction(\n",
    "    model=\"nomic-embed-text\",\n",
    "    url=\"http://127.0.0.1:11434/api/embeddings\"\n",
    ")\n",
    "\n",
    "def create_collection(name):\n",
    "    return client.create_collection(name=name, embedding_function=nomic_ef)\n",
    "\n",
    "def load_csv_to_chroma(csv_path, collection, batch_size=100):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    docs, ids, metas = [], [], []\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        accession = str(row.get(\"Run\", \"NA\"))\n",
    "\n",
    "        # Choose text fields dynamically depending on dataset\n",
    "        if \"RESISTANCE\" in df.columns:   # resfinder\n",
    "            text = f\"\"\"\n",
    "            Gene: {row.get('GENE','')}\n",
    "            Product: {row.get('PRODUCT','')}\n",
    "            Resistance: {row.get('RESISTANCE','')}\n",
    "            Organism: {row.get('Organism','')}\n",
    "            Database: {row.get('DATABASE','')}\n",
    "            AccessionID: {row.get('ACCESSION','')}\n",
    "            %Identity: {row.get('%IDENTITY','')}\n",
    "            %Coverage: {row.get('%COVERAGE','')}\n",
    "            \"\"\"\n",
    "        elif \"vfdb\" in csv_path:  # vfdb\n",
    "            text = f\"\"\"\n",
    "            Gene: {row.get('GENE','')}\n",
    "            Product: {row.get('PRODUCT','')}\n",
    "            Organism: {row.get('Organism','')}\n",
    "            Database: {row.get('DATABASE','')}\n",
    "            AccessionID: {row.get('ACCESSION','')}\n",
    "            %Identity: {row.get('%IDENTITY','')}\n",
    "            %Coverage: {row.get('%COVERAGE','')}\n",
    "            \"\"\"\n",
    "        elif \"plasmidfinder\" in csv_path:  # plasmidfinder\n",
    "            text = f\"\"\"\n",
    "            Gene: {row.get('GENE','')}\n",
    "            Product: {row.get('PRODUCT','')}\n",
    "            Organism: {row.get('Organism','')}\n",
    "            Database: {row.get('DATABASE','')}\n",
    "            AccessionID: {row.get('ACCESSION','')}\n",
    "            %Identity: {row.get('%IDENTITY','')}\n",
    "            %Coverage: {row.get('%COVERAGE','')}\n",
    "            \"\"\"\n",
    "        elif \"mge\" in csv_path.lower():  # mobile genetic elements\n",
    "            text = f\"\"\"\n",
    "            Name: {row.get('name','')}\n",
    "            Synonyms: {row.get('synonyms','')}\n",
    "            Type: {row.get('type','')}\n",
    "            Prediction: {row.get('prediction','')}\n",
    "            Organism: {row.get('Organism','')}\n",
    "            Identity: {row.get('identity','')}\n",
    "            Coverage: {row.get('coverage','')}\n",
    "            \"\"\"\n",
    "        else:\n",
    "            text = str(row.to_dict())\n",
    "\n",
    "        # Always keep accession + basic metadata\n",
    "        docs.append(text.strip())\n",
    "        ids.append(f\"{accession}_{i}\")\n",
    "        metas.append({\n",
    "            \"accession\": accession,\n",
    "            \"gene_db_acc\": str(row.get(\"ACCESSION\", \"\")),\n",
    "            \"collection_date\": str(row.get(\"Collection_Date\", \"\")),\n",
    "            \"country\": str(row.get(\"geo_loc_name_country\", \"\")),\n",
    "            \"continent\": str(row.get(\"geo_loc_name_country_continent\", \"\")),\n",
    "            \"host\": str(row.get(\"HOST\", \"\")),\n",
    "            \"organism\": str(row.get(\"Organism\", \"\")),\n",
    "        })\n",
    "\n",
    "        if len(docs) >= batch_size:\n",
    "            collection.add(documents=docs, ids=ids, metadatas=metas)\n",
    "            docs, ids, metas = [], [], []\n",
    "\n",
    "    if docs:\n",
    "        collection.add(documents=docs, ids=ids, metadatas=metas)\n",
    "\n",
    "# === Build all four collections ===\n",
    "resfinder_col = create_collection(\"resfinder\")\n",
    "vfdb_col = create_collection(\"vfdb\")\n",
    "plasmid_col = create_collection(\"plasmidfinder\")\n",
    "mge_col = create_collection(\"mge\")\n",
    "\n",
    "load_csv_to_chroma(\"/nobackup/zxww47/Resfinder/resfinder_combined.csv\", resfinder_col)\n",
    "load_csv_to_chroma(\"/nobackup/zxww47/Vfdb/vfdb_combined.csv\", vfdb_col)\n",
    "load_csv_to_chroma(\"/nobackup/zxww47/Plasmids/plasmidfinder_combined.csv\", plasmid_col)\n",
    "load_csv_to_chroma(\"/nobackup/zxww47/flye_mobile_elements_copy/merged_mge.csv\", mge_col)\n",
    "\n",
    "print(\"âœ… All collections rebuilt with Nomic embeddings\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sra_env_cpython)",
   "language": "python",
   "name": "sra_env_cpython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
